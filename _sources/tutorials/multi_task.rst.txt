
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/multi_task.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_multi_task.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_multi_task.py:


Task-specific policy in multi-task environments
================================================
This tutorial details how multi-task policies and batched environments can be used.

.. GENERATED FROM PYTHON SOURCE LINES 8-11

At the end of this tutorial, you will be capable of writing policies that
can compute actions in diverse settings using a distinct set of weights.
You will also be able to execute diverse environments in parallel.

.. GENERATED FROM PYTHON SOURCE LINES 11-15

.. code-block:: default


    import torch
    from torch import nn








.. GENERATED FROM PYTHON SOURCE LINES 16-21

.. code-block:: default


    from torchrl.envs import CatTensors, Compose, DoubleToFloat, ParallelEnv, TransformedEnv
    from torchrl.envs.libs.dm_control import DMControlEnv
    from torchrl.modules import MLP, TensorDictModule, TensorDictSequential








.. GENERATED FROM PYTHON SOURCE LINES 22-24

We design two environments, one humanoid that must complete the stand task
and another that must learn to walk.

.. GENERATED FROM PYTHON SOURCE LINES 24-52

.. code-block:: default


    env1 = DMControlEnv("humanoid", "stand")
    env1_obs_keys = list(env1.observation_spec.keys())
    env1 = TransformedEnv(
        env1,
        Compose(
            CatTensors(env1_obs_keys, "observation_stand", del_keys=False),
            CatTensors(env1_obs_keys, "observation"),
            DoubleToFloat(
                in_keys=["observation_stand", "observation"],
                in_keys_inv=["action"],
            ),
        ),
    )
    env2 = DMControlEnv("humanoid", "walk")
    env2_obs_keys = list(env2.observation_spec.keys())
    env2 = TransformedEnv(
        env2,
        Compose(
            CatTensors(env2_obs_keys, "observation_walk", del_keys=False),
            CatTensors(env2_obs_keys, "observation"),
            DoubleToFloat(
                in_keys=["observation_walk", "observation"],
                in_keys_inv=["action"],
            ),
        ),
    )








.. GENERATED FROM PYTHON SOURCE LINES 53-62

.. code-block:: default


    tdreset1 = env1.reset()
    tdreset2 = env2.reset()

    # In TorchRL, stacking is done in a lazy manner: the original tensordicts
    # can still be recovered by indexing the main tensordict
    tdreset = torch.stack([tdreset1, tdreset2], 0)
    assert tdreset[0] is tdreset1








.. GENERATED FROM PYTHON SOURCE LINES 63-66

.. code-block:: default


    print(tdreset[0])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            done: Tensor(torch.Size([1]), dtype=torch.bool),
            observation: Tensor(torch.Size([67]), dtype=torch.float32),
            observation_stand: Tensor(torch.Size([67]), dtype=torch.float32)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 67-73

Policy
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We will design a policy where a backbone reads the "observation" key.
Then specific sub-components will ready the "observation_stand" and
"observation_walk" keys of the stacked tensordicts, if they are present,
and pass them through the dedicated sub-network.

.. GENERATED FROM PYTHON SOURCE LINES 73-76

.. code-block:: default


    action_dim = env1.action_spec.shape[-1]








.. GENERATED FROM PYTHON SOURCE LINES 77-95

.. code-block:: default


    policy_common = TensorDictModule(
        nn.Linear(67, 64), in_keys=["observation"], out_keys=["hidden"]
    )
    policy_stand = TensorDictModule(
        MLP(67 + 64, action_dim, depth=2),
        in_keys=["observation_stand", "hidden"],
        out_keys=["action"],
    )
    policy_walk = TensorDictModule(
        MLP(67 + 64, action_dim, depth=2),
        in_keys=["observation_walk", "hidden"],
        out_keys=["action"],
    )
    seq = TensorDictSequential(
        policy_common, policy_stand, policy_walk, partial_tolerant=True
    )








.. GENERATED FROM PYTHON SOURCE LINES 96-97

Let's check that our sequence outputs actions for a single env (stand).

.. GENERATED FROM PYTHON SOURCE LINES 97-100

.. code-block:: default


    seq(env1.reset())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            action: Tensor(torch.Size([21]), dtype=torch.float32),
            done: Tensor(torch.Size([1]), dtype=torch.bool),
            hidden: Tensor(torch.Size([64]), dtype=torch.float32),
            observation: Tensor(torch.Size([67]), dtype=torch.float32),
            observation_stand: Tensor(torch.Size([67]), dtype=torch.float32)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 101-102

Let's check that our sequence outputs actions for a single env (walk).

.. GENERATED FROM PYTHON SOURCE LINES 102-105

.. code-block:: default


    seq(env2.reset())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            action: Tensor(torch.Size([21]), dtype=torch.float32),
            done: Tensor(torch.Size([1]), dtype=torch.bool),
            hidden: Tensor(torch.Size([64]), dtype=torch.float32),
            observation: Tensor(torch.Size([67]), dtype=torch.float32),
            observation_walk: Tensor(torch.Size([67]), dtype=torch.float32)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 106-110

This also works with the stack: now the stand and walk keys have
disappeared, because they're not shared by all tensordicts. But the
``TensorDictSequential`` still performed the operations. Note that the
backbone was executed in a vectorized way - not in a loop - which is more efficient.

.. GENERATED FROM PYTHON SOURCE LINES 110-113

.. code-block:: default


    seq(tdreset)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    LazyStackedTensorDict(
        fields={
            action: Tensor(torch.Size([2, 21]), dtype=torch.float32),
            done: Tensor(torch.Size([2, 1]), dtype=torch.bool),
            hidden: Tensor(torch.Size([2, 64]), dtype=torch.float32),
            observation: Tensor(torch.Size([2, 67]), dtype=torch.float32)},
        batch_size=torch.Size([2]),
        device=cpu,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 114-124

Executing diverse tasks in parallel
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We can parallelize the operations if the common keys-value pairs share the
same specs (in particular their shape and dtype must match: you can't do the
following if the observation shapes are different but are pointed to by the
same key).

If ParallelEnv receives a single env making function, it will assume that
a single task has to be performed. If a list of functions is provided, then
it will assume that we are in a multi-task setting.

.. GENERATED FROM PYTHON SOURCE LINES 124-162

.. code-block:: default



    def env1_maker():
        return TransformedEnv(
            DMControlEnv("humanoid", "stand"),
            Compose(
                CatTensors(env1_obs_keys, "observation_stand", del_keys=False),
                CatTensors(env1_obs_keys, "observation"),
                DoubleToFloat(
                    in_keys=["observation_stand", "observation"],
                    in_keys_inv=["action"],
                ),
            ),
        )


    def env2_maker():
        return TransformedEnv(
            DMControlEnv("humanoid", "walk"),
            Compose(
                CatTensors(env2_obs_keys, "observation_walk", del_keys=False),
                CatTensors(env2_obs_keys, "observation"),
                DoubleToFloat(
                    in_keys=["observation_walk", "observation"],
                    in_keys_inv=["action"],
                ),
            ),
        )


    env = ParallelEnv(2, [env1_maker, env2_maker])
    assert not env._single_task

    tdreset = env.reset()
    print(tdreset)
    print(tdreset[0])
    print(tdreset[1])  # should be different





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    LazyStackedTensorDict(
        fields={
            done: Tensor(torch.Size([2, 1]), dtype=torch.bool),
            observation: Tensor(torch.Size([2, 67]), dtype=torch.float32)},
        batch_size=torch.Size([2]),
        device=cpu,
        is_shared=False)
    TensorDict(
        fields={
            done: Tensor(torch.Size([1]), dtype=torch.bool),
            observation: Tensor(torch.Size([67]), dtype=torch.float32),
            observation_stand: Tensor(torch.Size([67]), dtype=torch.float32)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)
    TensorDict(
        fields={
            done: Tensor(torch.Size([1]), dtype=torch.bool),
            observation: Tensor(torch.Size([67]), dtype=torch.float32),
            observation_walk: Tensor(torch.Size([67]), dtype=torch.float32)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 163-164

Let's pass the output through our network.

.. GENERATED FROM PYTHON SOURCE LINES 164-170

.. code-block:: default


    tdreset = seq(tdreset)
    print(tdreset)
    print(tdreset[0])
    print(tdreset[1])  # should be different but all have an "action" key





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    LazyStackedTensorDict(
        fields={
            action: Tensor(torch.Size([2, 21]), dtype=torch.float32),
            done: Tensor(torch.Size([2, 1]), dtype=torch.bool),
            hidden: Tensor(torch.Size([2, 64]), dtype=torch.float32),
            observation: Tensor(torch.Size([2, 67]), dtype=torch.float32)},
        batch_size=torch.Size([2]),
        device=cpu,
        is_shared=False)
    TensorDict(
        fields={
            action: Tensor(torch.Size([21]), dtype=torch.float32),
            done: Tensor(torch.Size([1]), dtype=torch.bool),
            hidden: Tensor(torch.Size([64]), dtype=torch.float32),
            observation: Tensor(torch.Size([67]), dtype=torch.float32),
            observation_stand: Tensor(torch.Size([67]), dtype=torch.float32)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)
    TensorDict(
        fields={
            action: Tensor(torch.Size([21]), dtype=torch.float32),
            done: Tensor(torch.Size([1]), dtype=torch.bool),
            hidden: Tensor(torch.Size([64]), dtype=torch.float32),
            observation: Tensor(torch.Size([67]), dtype=torch.float32),
            observation_walk: Tensor(torch.Size([67]), dtype=torch.float32)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 171-177

.. code-block:: default


    env.step(tdreset)  # computes actions and execute steps in parallel
    print(tdreset)
    print(tdreset[0])
    print(tdreset[1])  # next_observation has now been written





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    LazyStackedTensorDict(
        fields={
            action: Tensor(torch.Size([2, 21]), dtype=torch.float32),
            done: Tensor(torch.Size([2, 1]), dtype=torch.bool),
            hidden: Tensor(torch.Size([2, 64]), dtype=torch.float32),
            next: LazyStackedTensorDict(
                fields={
                    observation: Tensor(torch.Size([2, 67]), dtype=torch.float32)},
                batch_size=torch.Size([2]),
                device=cpu,
                is_shared=False),
            observation: Tensor(torch.Size([2, 67]), dtype=torch.float32),
            reward: Tensor(torch.Size([2, 1]), dtype=torch.float64)},
        batch_size=torch.Size([2]),
        device=cpu,
        is_shared=False)
    TensorDict(
        fields={
            action: Tensor(torch.Size([21]), dtype=torch.float32),
            done: Tensor(torch.Size([1]), dtype=torch.bool),
            hidden: Tensor(torch.Size([64]), dtype=torch.float32),
            next: TensorDict(
                fields={
                    observation: Tensor(torch.Size([67]), dtype=torch.float32),
                    observation_stand: Tensor(torch.Size([67]), dtype=torch.float32)},
                batch_size=torch.Size([]),
                device=cpu,
                is_shared=False),
            observation: Tensor(torch.Size([67]), dtype=torch.float32),
            observation_stand: Tensor(torch.Size([67]), dtype=torch.float32),
            reward: Tensor(torch.Size([1]), dtype=torch.float64)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)
    TensorDict(
        fields={
            action: Tensor(torch.Size([21]), dtype=torch.float32),
            done: Tensor(torch.Size([1]), dtype=torch.bool),
            hidden: Tensor(torch.Size([64]), dtype=torch.float32),
            next: TensorDict(
                fields={
                    observation: Tensor(torch.Size([67]), dtype=torch.float32),
                    observation_walk: Tensor(torch.Size([67]), dtype=torch.float32)},
                batch_size=torch.Size([]),
                device=cpu,
                is_shared=False),
            observation: Tensor(torch.Size([67]), dtype=torch.float32),
            observation_walk: Tensor(torch.Size([67]), dtype=torch.float32),
            reward: Tensor(torch.Size([1]), dtype=torch.float64)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 178-180

Rollout
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 180-183

.. code-block:: default


    td_rollout = env.rollout(100, policy=seq, return_contiguous=False)








.. GENERATED FROM PYTHON SOURCE LINES 184-187

.. code-block:: default


    td_rollout[:, 0]  # tensordict of the first step: only the common keys are shown





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    LazyStackedTensorDict(
        fields={
            action: Tensor(torch.Size([2, 21]), dtype=torch.float32),
            done: Tensor(torch.Size([2, 1]), dtype=torch.bool),
            hidden: Tensor(torch.Size([2, 64]), dtype=torch.float32),
            next: LazyStackedTensorDict(
                fields={
                    observation: Tensor(torch.Size([2, 67]), dtype=torch.float32)},
                batch_size=torch.Size([2]),
                device=cpu,
                is_shared=False),
            observation: Tensor(torch.Size([2, 67]), dtype=torch.float32),
            reward: Tensor(torch.Size([2, 1]), dtype=torch.float64)},
        batch_size=torch.Size([2]),
        device=cpu,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 188-190

.. code-block:: default


    td_rollout[0]  # tensordict of the first env: the stand obs is present




.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    LazyStackedTensorDict(
        fields={
            action: Tensor(torch.Size([100, 21]), dtype=torch.float32),
            done: Tensor(torch.Size([100, 1]), dtype=torch.bool),
            hidden: Tensor(torch.Size([100, 64]), dtype=torch.float32),
            next: LazyStackedTensorDict(
                fields={
                    observation: Tensor(torch.Size([100, 67]), dtype=torch.float32),
                    observation_stand: Tensor(torch.Size([100, 67]), dtype=torch.float32)},
                batch_size=torch.Size([100]),
                device=cpu,
                is_shared=False),
            observation: Tensor(torch.Size([100, 67]), dtype=torch.float32),
            observation_stand: Tensor(torch.Size([100, 67]), dtype=torch.float32),
            reward: Tensor(torch.Size([100, 1]), dtype=torch.float64)},
        batch_size=torch.Size([100]),
        device=cpu,
        is_shared=False)




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  3.684 seconds)


.. _sphx_glr_download_tutorials_multi_task.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: multi_task.py <multi_task.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: multi_task.ipynb <multi_task.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
