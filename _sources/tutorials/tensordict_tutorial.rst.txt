
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/tensordict_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_tensordict_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_tensordict_tutorial.py:


TensorDict
============================
``TensorDict`` is a new tensor structure introduced in TorchRL.

.. GENERATED FROM PYTHON SOURCE LINES 8-33

With RL, you need to be able to deal with multiple tensors such as actions,
observations and reward. ``TensorDict`` makes it more convenient to deal
with multiple tensors at the same time for operations such as casting to
device, reshaping, stacking etc.

Furthermore, different RL algorithms can deal with different input and
outputs. The ``TensorDict`` class makes it possible to abstract away the
differences between these algorithms.

TensorDict combines the convenience of using ``dicts`` to organize your
data with the power of pytorch tensors.

Improving the modularity of codes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Let's suppose we have 2 datasets: Dataset A which has images and labels and
Dataset B which has images, segmentation maps and labels.

Suppose we want to train a common algorithm over these two datasets (i.e. an
algorithm that would ignore the mask or infer it when needed).

In classical pytorch we would need to do the following:

TODO: should these snippets be re-formatted?

**Method A**

.. GENERATED FROM PYTHON SOURCE LINES 33-41

.. code-block:: default


    # for i in range(optim_steps):
    #   images, labels = get_data_A()
    #   loss = loss_module(images, labels)
    #   loss.backward()
    #   optim.step()
    #   optim.zero_grad()








.. GENERATED FROM PYTHON SOURCE LINES 42-43

**Method B**

.. GENERATED FROM PYTHON SOURCE LINES 43-51

.. code-block:: default


    # for i in range(optim_steps):
    #   images, labels = get_data_B()
    #   loss = loss_module(images, labels)
    #   loss.backward()
    #   optim.step()
    #   optim.zero_grad()








.. GENERATED FROM PYTHON SOURCE LINES 52-55

We can see that this limits the reusability of code. A lot of code has to be
rewriten because of the modality difference between the 2 datasets.
The idea of TensorDict is to do the following:

.. GENERATED FROM PYTHON SOURCE LINES 57-58

**General Method**

.. GENERATED FROM PYTHON SOURCE LINES 58-66

.. code-block:: default


    # for i in range(optim_steps):
    #   images, labels = get_data()
    #   loss = loss_module(images, labels)
    #   loss.backward()
    #   optim.step()
    #   optim.zero_grad()








.. GENERATED FROM PYTHON SOURCE LINES 67-74

We can now reuse the same training loop across datasets and losses.

Can't I do this with a python dict?
--------------------------------------

One could argue that you could achieve the same results with a dataset
that outputs a pytorch dict.

.. GENERATED FROM PYTHON SOURCE LINES 74-81

.. code-block:: default


    # class DictDataset(Dataset):
    #   ...
    #   def __getitem__(self, idx)
    #       ...
    #       return {"images": image, "masks": mask}








.. GENERATED FROM PYTHON SOURCE LINES 82-84

However to achieve this you would need to write a complicated collate
function that make sure that every modality is aggregated properly.

.. GENERATED FROM PYTHON SOURCE LINES 84-96

.. code-block:: default



    def collate_dict_fn(dict_list):
        final_dict = {}
        for key in dict_list[0].keys():
            final_dict[key] = []
            for single_dict in dict_list:
                final_dict[key].append(single_dict[key])
            final_dict[key] = torch.stack(final_dict[key], dim=0)
        return final_dict









.. GENERATED FROM PYTHON SOURCE LINES 97-100

With TensorDicts this is now much simpler:

**dataloader = Dataloader(DictDataset(), collate_fn = collate_dict_fn)**

.. GENERATED FROM PYTHON SOURCE LINES 100-107

.. code-block:: default


    # class DictDataset(Dataset):
    #   ...
    #   def __getitem__(self, idx)
    #       ...
    #       return TensorDict({"images": image, "masks": mask})








.. GENERATED FROM PYTHON SOURCE LINES 108-122

Here, the collate function is as simple as:

**collate_tensordict_fn = lambda tds : torch.stack(tds, dim=0)**

**dataloader = Dataloader(DictDataset(), collate_fn = collate_tensordict_fn)**

This is even more useful when considering nested structures
(Which ``TensorDict`` supports).

TensorDict inherits multiple properties from ``torch.Tensor`` and ``dict``
that we will detail furtherdown.

TensorDict structure
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 122-125

.. code-block:: default


    import torch








.. GENERATED FROM PYTHON SOURCE LINES 126-134

.. code-block:: default


    from tensordict.tensordict import (
        _PermutedTensorDict,
        _UnsqueezedTensorDict,
        _ViewedTensorDict,
        TensorDict,
    )








.. GENERATED FROM PYTHON SOURCE LINES 135-143

TensorDict is a Datastructure indexed by either keys or numerical indices.
The values can either be tensors, memory-mapped tensors or ``TensorDict``. The
values need to share the same memory location (device or shared memory).
They can however have different dtypes.

Another essential property of TensorDict is the ``batch_size`` (or ``shape``)
which is defined as the n-first dimensions of the tensors. It must be common
across values, and it must be set explicitly when instantiating a ``TensorDict``.

.. GENERATED FROM PYTHON SOURCE LINES 143-158

.. code-block:: default


    a = torch.zeros(3, 4)
    b = torch.zeros(3, 4, 5)

    # works
    tensordict = TensorDict({"a": a, "b": b}, batch_size=[3, 4])
    tensordict = TensorDict({"a": a, "b": b}, batch_size=[3])
    tensordict = TensorDict({"a": a, "b": b}, batch_size=[])

    # does not work
    try:
        tensordict = TensorDict({"a": a, "b": b}, batch_size=[3, 4, 5])
    except RuntimeError:
        print("caramba!")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    caramba!




.. GENERATED FROM PYTHON SOURCE LINES 159-162

Nested ``TensorDict`` have therefore the following property: the parent
``TensorDict`` needs to have a batch_size included in the childs
``TensorDict`` batch size.

.. GENERATED FROM PYTHON SOURCE LINES 162-174

.. code-block:: default


    a = torch.zeros(3, 4)
    b = TensorDict(
        {
            "c": torch.zeros(3, 4, 5, dtype=torch.int32),
            "d": torch.zeros(3, 4, 5, 6, dtype=torch.float32),
        },
        batch_size=[3, 4, 5],
    )
    tensordict = TensorDict({"a": a, "b": b}, batch_size=[3, 4])
    print(tensordict)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            a: Tensor(torch.Size([3, 4, 1]), dtype=torch.float32),
            b: TensorDict(
                fields={
                    c: Tensor(torch.Size([3, 4, 5, 1]), dtype=torch.int32),
                    d: Tensor(torch.Size([3, 4, 5, 6]), dtype=torch.float32)},
                batch_size=torch.Size([3, 4, 5]),
                device=None,
                is_shared=False)},
        batch_size=torch.Size([3, 4]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 175-180

``TensorDict`` does not support algebraic operations by design.

TensorDict Dictionary Features
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
``TensorDict`` shares a lot of features with python dictionaries.

.. GENERATED FROM PYTHON SOURCE LINES 180-186

.. code-block:: default


    a = torch.zeros(3, 4, 5)
    b = torch.zeros(3, 4)
    tensordict = TensorDict({"a": a, "b": b}, batch_size=[3, 4])
    print(tensordict)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32),
            b: Tensor(torch.Size([3, 4, 1]), dtype=torch.float32)},
        batch_size=torch.Size([3, 4]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 187-191

``get(key)``
------------------------------
If we want to access a certain key, we can index the tensordict
or alternatively use the ``get`` method:

.. GENERATED FROM PYTHON SOURCE LINES 191-195

.. code-block:: default


    print("get and __getitem__ match:", tensordict["a"] is tensordict.get("a") is a)
    print(tensordict["a"].shape)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    get and __getitem__ match: True
    torch.Size([3, 4, 5])




.. GENERATED FROM PYTHON SOURCE LINES 196-197

The ``get`` method also supports default values:

.. GENERATED FROM PYTHON SOURCE LINES 197-201

.. code-block:: default


    out = tensordict.get("foo", torch.ones(3))
    print(out)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([1., 1., 1.])




.. GENERATED FROM PYTHON SOURCE LINES 202-206

``set(key, value)``
------------------------------
The ``set()`` method can be used to set new values.
Regular indexing also does the job:

.. GENERATED FROM PYTHON SOURCE LINES 206-215

.. code-block:: default


    c = torch.zeros((3, 4, 2, 2))
    tensordict.set("c", c)
    print(f"td[\"c\"] is c: {c is tensordict['c']}")

    d = torch.zeros((3, 4, 2, 2))
    tensordict["d"] = d
    print(f"td[\"d\"] is d: {d is tensordict['d']}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    td["c"] is c: True
    td["d"] is d: True




.. GENERATED FROM PYTHON SOURCE LINES 216-219

``keys()``
------------------------------
We can access the keys of a tensordict:

.. GENERATED FROM PYTHON SOURCE LINES 219-225

.. code-block:: default


    tensordict["c"] = torch.zeros(tensordict.shape)
    tensordict.set("d", torch.ones(tensordict.shape))
    assert (tensordict["c"] == 0).all()
    assert (tensordict["d"] == 1).all()








.. GENERATED FROM PYTHON SOURCE LINES 226-231

``values()``
------------------------------
The values of a ``TensorDict`` can be retrieved with the ``values()`` function.
Note that, unlike python ``dicts``, the ``values()`` method returns a
generator and not a list.

.. GENERATED FROM PYTHON SOURCE LINES 231-235

.. code-block:: default


    for value in tensordict.values():
        print(value.shape)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    torch.Size([3, 4, 5])
    torch.Size([3, 4, 1])
    torch.Size([3, 4, 1])
    torch.Size([3, 4, 1])




.. GENERATED FROM PYTHON SOURCE LINES 236-240

``update(tensordict_or_dict)``
------------------------------
The ``update`` method can be used to update a TensorDict with another one
(or with a dict):

.. GENERATED FROM PYTHON SOURCE LINES 240-247

.. code-block:: default


    tensordict.update({"a": torch.ones((3, 4, 5)), "d": 2 * torch.ones((3, 4, 2))})
    # Also works with tensordict.update(TensorDict({"a":torch.ones((3, 4, 5)),
    # "c":torch.ones((3, 4, 2))}, batch_size=[3,4]))
    print(f"a is now equal to 1: {(tensordict['a'] == 1).all()}")
    print(f"d is now equal to 2: {(tensordict['d'] == 2).all()}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    a is now equal to 1: True
    d is now equal to 2: True




.. GENERATED FROM PYTHON SOURCE LINES 248-251

``del``
------------------------------
TensorDict also support keys deletion with the ``del`` operator:

.. GENERATED FROM PYTHON SOURCE LINES 251-256

.. code-block:: default


    print("before")
    for k in tensordict.keys():
        print(k)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    before
    a
    b
    c
    d




.. GENERATED FROM PYTHON SOURCE LINES 257-263

.. code-block:: default


    del tensordict["c"]
    print("after")
    for k in tensordict.keys():
        print(k)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    after
    a
    b
    d




.. GENERATED FROM PYTHON SOURCE LINES 264-276

TensorDict tensor features
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
On many regards, TensorDict is a Tensor-like class: a great deal of tensor
operation also work on tensordicts, making it easy to cast them across
multiple tensors.

Batch size
------------------------------
``TensorDict`` has a batch size which is shared across all tensors. The batch
size can be [], unidimensional or multidimensional according to your needs,
but it must be shared across tensors. Indeed, you cannot have items that don't
share the batch size inside the same TensorDict:

.. GENERATED FROM PYTHON SOURCE LINES 276-282

.. code-block:: default


    tensordict = TensorDict(
        {"a": torch.zeros(3, 4, 5), "b": torch.zeros(3, 4)}, batch_size=[3, 4]
    )
    print(f"Our TensorDict is of size {tensordict.shape}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Our TensorDict is of size torch.Size([3, 4])




.. GENERATED FROM PYTHON SOURCE LINES 283-284

The batch size can be changed if needed:

.. GENERATED FROM PYTHON SOURCE LINES 284-291

.. code-block:: default


    # we cannot add tensors that violate the batch size:
    try:
        tensordict.update({"c": torch.zeros(4, 3, 1)})
    except RuntimeError as err:
        print(f"Caramba! We got this error: {err}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Caramba! We got this error: batch dimension mismatch, got self.batch_size=torch.Size([3, 4]) and tensor.shape[:self.batch_dims]=torch.Size([4, 3]) with tensor tensor([[[0.],
             [0.],
             [0.]],

            [[0.],
             [0.],
             [0.]],

            [[0.],
             [0.],
             [0.]],

            [[0.],
             [0.],
             [0.]]])




.. GENERATED FROM PYTHON SOURCE LINES 292-293

but it must comply with the tensor shapes:

.. GENERATED FROM PYTHON SOURCE LINES 293-298

.. code-block:: default


    tensordict.batch_size = [3]
    assert tensordict.batch_size == torch.Size([3])
    tensordict.batch_size = [3, 4]








.. GENERATED FROM PYTHON SOURCE LINES 299-305

.. code-block:: default


    try:
        tensordict.batch_size = [4, 4]
    except RuntimeError as err:
        print(f"Caramba! We got this error: {err}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Caramba! We got this error: the tensor a has shape torch.Size([3, 4, 5]) which is incompatible with the new shape torch.Size([4, 4]).




.. GENERATED FROM PYTHON SOURCE LINES 306-307

We can also fill the values of a TensorDict sequentially

.. GENERATED FROM PYTHON SOURCE LINES 307-313

.. code-block:: default


    tensordict = TensorDict({}, [10])
    for i in range(10):
        tensordict[i] = TensorDict({"a": torch.randn(3, 4)}, [])
    print(tensordict)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            a: Tensor(torch.Size([10, 3, 4]), dtype=torch.float32)},
        batch_size=torch.Size([10]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 314-315

If all values are not filled, they get the default value of zero.

.. GENERATED FROM PYTHON SOURCE LINES 315-324

.. code-block:: default


    tensordict = TensorDict({}, [10])
    for i in range(2):
        tensordict[i] = TensorDict({"a": torch.randn(3, 4)}, [])
    assert (tensordict[9]["a"] == torch.zeros((3, 4))).all()
    tensordict = TensorDict(
        {"a": torch.zeros(3, 4, 5), "b": torch.zeros(3, 4)}, batch_size=[3, 4]
    )








.. GENERATED FROM PYTHON SOURCE LINES 325-344

Devices
------------------------------
TensorDict can be sent to the desired devices like a pytorch tensor with
``td.cuda()`` or ``td.to(device)`` with ``device`` the desired device.

Memory sharing via physical memory usage
------------------------------
When on cpu, one can use either ``tensordict.memmap_()`` or
``tensordict.share_memory_()`` to send a ``tensordict`` to represent it as
a memory-mapped collection of tensors or put it in shared memory resp.

Tensor operations
------------------------------
We can perform tensor operations among the batch dimensions:

**Cloning**

TensorDict supports cloning. Cloning returns the same TensorDict class
than the original item.

.. GENERATED FROM PYTHON SOURCE LINES 344-350

.. code-block:: default


    tensordict_clone = tensordict.clone()
    print(
        f"Content is identical ({(tensordict['a'] == tensordict_clone['a']).all()}) but duplicated ({tensordict['a'] is not tensordict_clone['a']})"
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Content is identical (True) but duplicated (True)




.. GENERATED FROM PYTHON SOURCE LINES 351-354

**Slicing and Indexing**

Slicing and indexing is supported along the batch dimensions.

.. GENERATED FROM PYTHON SOURCE LINES 354-357

.. code-block:: default


    print(tensordict[0])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            a: Tensor(torch.Size([4, 5]), dtype=torch.float32),
            b: Tensor(torch.Size([4, 1]), dtype=torch.float32)},
        batch_size=torch.Size([4]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 358-361

.. code-block:: default


    print(tensordict[1:])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            a: Tensor(torch.Size([2, 4, 5]), dtype=torch.float32),
            b: Tensor(torch.Size([2, 4, 1]), dtype=torch.float32)},
        batch_size=torch.Size([2, 4]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 362-365

.. code-block:: default


    print(tensordict[:, 2:])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            a: Tensor(torch.Size([3, 2, 5]), dtype=torch.float32),
            b: Tensor(torch.Size([3, 2, 1]), dtype=torch.float32)},
        batch_size=torch.Size([3, 2]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 366-376

**Setting Values with Indexing**

In general, ``tensodict[tuple_index] = new_tensordict`` will work as long as
the batch sizes match.

If one wants to build a tensordict that keeps track of the original tensordict,
the ``get_sub_tensordict`` method can be used: in that case, a
``SubTensorDict`` instance will be returned. This class will store a pointer
to the original tensordict as well as the desired index such that tensor
modifications can be achieved easily.

.. GENERATED FROM PYTHON SOURCE LINES 376-385

.. code-block:: default


    tensordict = TensorDict(
        {"a": torch.zeros(3, 4, 5), "b": torch.zeros(3, 4)}, batch_size=[3, 4]
    )
    # a SubTensorDict keeps track of the original one: it does not create a copy in memory of the original data
    subtd = tensordict.get_sub_tensordict((slice(None), torch.tensor([1, 3])))
    tensordict.fill_("a", -1)
    assert (subtd["a"] == -1).all(), subtd["a"]  # the "a" key-value pair has changed








.. GENERATED FROM PYTHON SOURCE LINES 386-387

We can set values easily just by indexing the tensordict:

.. GENERATED FROM PYTHON SOURCE LINES 387-392

.. code-block:: default


    td2 = TensorDict({"a": torch.zeros(2, 4, 5), "b": torch.zeros(2, 4)}, batch_size=[2, 4])
    tensordict[:-1] = td2
    print(tensordict["a"], tensordict["b"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[[ 0.,  0.,  0.,  0.,  0.],
             [ 0.,  0.,  0.,  0.,  0.],
             [ 0.,  0.,  0.,  0.,  0.],
             [ 0.,  0.,  0.,  0.,  0.]],

            [[ 0.,  0.,  0.,  0.,  0.],
             [ 0.,  0.,  0.,  0.,  0.],
             [ 0.,  0.,  0.,  0.,  0.],
             [ 0.,  0.,  0.,  0.,  0.]],

            [[-1., -1., -1., -1., -1.],
             [-1., -1., -1., -1., -1.],
             [-1., -1., -1., -1., -1.],
             [-1., -1., -1., -1., -1.]]]) tensor([[[0.],
             [0.],
             [0.],
             [0.]],

            [[0.],
             [0.],
             [0.],
             [0.]],

            [[0.],
             [0.],
             [0.],
             [0.]]])




.. GENERATED FROM PYTHON SOURCE LINES 393-396

**Masking**

We mask ``TensorDict`` as we mask tensors.

.. GENERATED FROM PYTHON SOURCE LINES 396-400

.. code-block:: default


    mask = torch.BoolTensor([[1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0]])
    tensordict[mask]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            a: Tensor(torch.Size([6, 5]), dtype=torch.float32),
            b: Tensor(torch.Size([6, 1]), dtype=torch.float32)},
        batch_size=torch.Size([6]),
        device=None,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 401-405

**Stacking**

``TensorDict`` supports stacking. By default, stacking is done in a lazy
fashion, returning a ``LazyStackedTensorDict`` item.

.. GENERATED FROM PYTHON SOURCE LINES 405-415

.. code-block:: default


    # Stack
    clonned_tensordict = tensordict.clone()
    staked_tensordict = torch.stack([tensordict, clonned_tensordict], dim=0)
    print(staked_tensordict)

    # indexing a lazy stack returns the original tensordicts
    if staked_tensordict[0] is tensordict and staked_tensordict[1] is clonned_tensordict:
        print("every tensordict is awesome!")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    LazyStackedTensorDict(
        fields={
            a: Tensor(torch.Size([2, 3, 4, 5]), dtype=torch.float32),
            b: Tensor(torch.Size([2, 3, 4, 1]), dtype=torch.float32)},
        batch_size=torch.Size([2, 3, 4]),
        device=None,
        is_shared=False)
    every tensordict is awesome!




.. GENERATED FROM PYTHON SOURCE LINES 416-419

If we want to have a contiguous tensordict, we can call ``.to_tensordict()``
or ``.contiguous()``. It is recommended to perform this operation before
accessing the values of the stacked tensordict for efficiency purposes.

.. GENERATED FROM PYTHON SOURCE LINES 419-423

.. code-block:: default


    assert isinstance(staked_tensordict.contiguous(), TensorDict)
    assert isinstance(staked_tensordict.to_tensordict(), TensorDict)








.. GENERATED FROM PYTHON SOURCE LINES 424-427

**Unbind**

TensorDict can be unbound along a dim over the tensordict batch size.

.. GENERATED FROM PYTHON SOURCE LINES 427-433

.. code-block:: default


    list_tensordict = tensordict.unbind(0)
    assert type(list_tensordict) == tuple
    assert len(list_tensordict) == 3
    assert (torch.stack(list_tensordict, dim=0).contiguous() == tensordict).all()








.. GENERATED FROM PYTHON SOURCE LINES 434-438

**Cat**

TensorDict supports cat to concatenate among a dim. The dim must be lower
than the ``batch_dims`` (i.e. the length of the batch_size).

.. GENERATED FROM PYTHON SOURCE LINES 438-442

.. code-block:: default


    list_tensordict = tensordict.unbind(0)
    assert torch.cat(list_tensordict, dim=0).shape[0] == 12








.. GENERATED FROM PYTHON SOURCE LINES 443-447

**View**

Support for the view operation returning a ``_ViewedTensorDict``.
Use ``to_tensordict`` to comeback to retrieve TensorDict.

.. GENERATED FROM PYTHON SOURCE LINES 447-451

.. code-block:: default


    assert type(tensordict.view(-1)) == _ViewedTensorDict
    assert tensordict.view(-1).shape[0] == 12








.. GENERATED FROM PYTHON SOURCE LINES 452-456

**Permute**

We can permute the dims of ``TensorDict``. Permute is a Lazy operation that
returns _PermutedTensorDict. Use ``to_tensordict`` to convert to ``TensorDict``.

.. GENERATED FROM PYTHON SOURCE LINES 456-460

.. code-block:: default


    assert type(tensordict.permute(1, 0)) == _PermutedTensorDict
    assert tensordict.permute(1, 0).batch_size == torch.Size([4, 3])








.. GENERATED FROM PYTHON SOURCE LINES 461-464

**Reshape**

Reshape allows reshaping the ``TensorDict`` batch size.

.. GENERATED FROM PYTHON SOURCE LINES 464-467

.. code-block:: default


    assert tensordict.reshape(-1).batch_size == torch.Size([12])








.. GENERATED FROM PYTHON SOURCE LINES 468-474

**Squeeze and Unsqueeze**

Tensordict also supports squeeze and unsqueeze. Unsqueeze is a lazy operation
that returns _UnsqueezedTensorDict. Use ``to_tensordict`` to retrieve a
tensordict after unsqueeze. Calling ``unsqueeze(dim).squeeze(dim)`` returns
the original tensordict.

.. GENERATED FROM PYTHON SOURCE LINES 474-482

.. code-block:: default


    unsqueezed_tensordict = tensordict.unsqueeze(0)
    assert type(unsqueezed_tensordict) == _UnsqueezedTensorDict
    assert unsqueezed_tensordict.batch_size == torch.Size([1, 3, 4])

    assert type(unsqueezed_tensordict.squeeze(0)) == TensorDict
    assert unsqueezed_tensordict.squeeze(0) is tensordict








.. GENERATED FROM PYTHON SOURCE LINES 483-484

Have fun with TensorDict!


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.028 seconds)


.. _sphx_glr_download_tutorials_tensordict_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tensordict_tutorial.py <tensordict_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tensordict_tutorial.ipynb <tensordict_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
