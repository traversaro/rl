{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Pendulum: Writing your environment and transforms with TorchRL\n\n**Author**: [Vincent Moens](https://github.com/vmoens)\n\nCreating an environment (a simulator or a interface to a physical control system)\nis an integrative part of reinforcement learning and control engineering.\n\nTorchRL provides a set of tools to do this in multiple contexts.\nThis tutorial demonstrates how to use PyTorch and :py:mod:`torchrl` code a pendulum\nsimulator from the ground up.\nIt is freely inspired by the Pendulum-v1 implementation from [OpenAI-Gym/Farama-Gymnasium\ncontrol library](https://github.com/Farama-Foundation/Gymnasium)_.\n\n.. figure:: /_static/img/pendulum.gif\n   :alt: Pendulum\n\n   Simple Pendulum\n\nKey learnings:\n\n- How to design an environment in TorchRL:\n  - Writing specs (input, observation and reward);\n  - Implementing behaviour: seeding, reset and step.\n- Transforming your environment inputs and outputs, and writing your own\n  transforms;\n- How to use :class:`tensordict.TensorDict` to carry arbitrary data structures\n  from sep to step.\n\nIn the process, we will touch three crucial components of TorchRL:\n\n* [environments](https://pytorch.org/rl/reference/envs.html)_\n* [transforms](https://pytorch.org/rl/reference/envs.html#transforms)_\n* [models (policy and value function)](https://pytorch.org/rl/reference/modules.html)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To give a sense of what can be achieved with TorchRL's environments, we will\nbe designing a *stateless* environment. While stateful environments keep track of\nthe latest physical state encountered and rely on this to simulate the state-to-state\ntransition, stateless environments expect the current state to be provided to\nthem at each step, along with the action undertaken. TorchRL supports both\ntypes of environments, but stateless environments are more generic and hence\ncover a broader range of features of the environment API in torchrl.\n\nModelling stateless environments gives users full control over the input and\noutputs of the simulator: one can reset an experiment at any stage. It also\nassumes that we have some control over a task, which may not always be the\ncase: solving a problem where we cannot control the current state is more\nchallenging but has a much wider set of applications.\n\nAnother advantage of stateless environments is that they can enable\nbatched execution of transition simulations. If the backend and the\nimplementation allow it, an algebraic operation can be executed seamlessly on\nscalars, vectors or tensors. This tutorial gives such examples.\n\nThis tutorial will be structured as follows:\n\n* We will first get acquainted with the environment properties:\n  its shape (``batch_size``), its methods (mainly :func:`EnvBase.step`,\n  :func:`EnvBase.reset` and :func:`EnvBase.set_seed`)\n  and finally its specs.\n* After having coded our simulator, we will demonstrate how it can be used\n  during training with transforms.\n* We will explore surprising new avenues that follow from the TorchRL's API,\n  including: the possibility of transforming inputs, the vectorized execution\n  of the simulation and the possibility of backpropagating through the\n  simulation graph.\n* Finally, will train a simple policy to solve the system we implemented.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport tqdm\nfrom tensordict.nn import TensorDictModule\nfrom tensordict.tensordict import TensorDict, TensorDictBase\nfrom torch import nn\n\nfrom torchrl.data import BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec\nfrom torchrl.envs import (\n    CatTensors,\n    EnvBase,\n    Transform,\n    TransformedEnv,\n    UnsqueezeTransform,\n)\nfrom torchrl.envs.transforms.transforms import _apply_to_composite\nfrom torchrl.envs.utils import check_env_specs, step_mdp\n\nDEFAULT_X = np.pi\nDEFAULT_Y = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are four things one must take care of when designing a new environment\nclass: :func:`EnvBase._reset`, which codes for the resetting of the simulator\nat a (potentially random) initial state, :func:`EnvBase._step` which codes\nfor the state transition dynamic, :func:`EnvBase._set_seed`` which\nimplements the seeding mechanism and, finally, the environment specs.\n\nLet us first describe the problem at hand: we would like to model a simple\npendulum, over which we can control the torque applied on its fixed point.\nOur goal is to place the pendulum in upward position (angular position at 0\nby convention) and having it standing still in that position.\nTo design our dynamic system, we need to define two equations: the motion\nequation following an action (the torque applied) and the reward equation\nthat will constitute our objective function.\n\nFor the motion equation, we will update the angular velocity following:\n\n\\begin{align}\\dot{\\theta}_{t+1} = \\dot{\\theta}_t + (3 * g / (2 * L) * \\sin(\\theta_t) + 3 / (m * L^2) * u) * dt\\end{align}\n\nwhere $\\dot{\\theta}$ is the angular velocity in rad/sec, $g$ is the\ngravitational force, $L$ is the pendulum length, $m$ is its mass,\n$\\theta$ is its angular position and $u$ is the torque. The\nangular position is then updated according to\n\n\\begin{align}\\theta_{t+1} = \\theta_{t} + \\dot{\\theta}_{t+1} dt\\end{align}\n\nWe define our reward as\n\n\\begin{align}r = -(\\theta^2 + 0.1 * \\dot{\\theta}^2 + 0.001 * u^2)\\end{align}\n\nwhich will be maximized when the angle is close to 0 (pendulum in upward\nposition), the angular velocity is close to 0 (no motion) and the torque is\n0 too.\n\n## Coding the effect of an action: :func:`torchrl.envs.EnvBase._step`\n\nThe step method is the first thing to consider, as it will encode\nthe simulation that is of interest to us. In TorchRL, the\n:class:`torchrl.envs.EnvBase` class has a :func:`EnvBase.step(tensordict)`\nmethod that receives a :class:`tensordict.TensorDict`\ninstance with an ``\"action\"`` entry indicating what action is to be taken.\n\nTo facilitate the reading and writing from that tensordict and to make sure\nthat the keys are consistent with what's expected from the library, the\nsimulation part has been delegated to a private abstract method :func:`_step`\nwhich reads input data from a tensordict, and writes a *new*  tensordict\nwith the output data.\n\nThe :func:`_step` method should do the following:\n\n  1. read the input keys (such as ``\"action\"``) and execute the simulation\n     based on these;\n  2. retrieve observations, done state and reward;\n  3. write the set of observation value along with the reward and done state\n     at the corresponding entries in a new :class:`TensorDict`.\n\nNext, the :func:`torchrl.envs.EnvBase.step` method will rearrange this\noutput and move the key-pair values of the observation in a new entry\nnamed ``\"next\"`` and leave the ``\"reward\"`` and ``\"done\"`` state at the\nroot level. It will also run some sanity checks on the shapes of the\ntensordict content.\n\nTypically, for stateful environments, this will look like\n\n```\n>>> policy(env.reset())\n>>> print(tensordict)\nTensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        observation: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n>>> env.step(tensordict)\n>>> print(tensordict)\nTensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                observation: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n            batch_size=torch.Size([]),\n            device=cpu,\n            is_shared=False),\n        reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        observation: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n```\nIn the Pendulum example, our :func:`_step` method will read the relevant entries\nfrom the input tensordict and compute the position and velocity of the\npendulum after the force encoded by the ``\"action\"`` key has been applied\nonto it. We compute the new angular position of the pendulum\n``new_th`` as the result of the previous position ``th`` plus the new\nvelocity ``new_thdot`` over a time interval ``dt``.\n\nSince our goal is to turn the pendulum up and maintain it still in that\nposition, our ``cost`` (negative reward) function is lower for positions\nclose to the target and low speeds.\nIndeed, we want to discourage positions that are far from being \"upward\"\nand/or speeds that are far from 0.\n\nIn our example, :func:`EnvBase._step` is encoded as a static method since our\nenvironment is stateless. In stateful settings, the ``self`` argument is\nneeded as the state needs to be read from the environment.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _step(tensordict):\n    th, thdot = tensordict[\"th\"], tensordict[\"thdot\"]  # th := theta\n\n    g_force = tensordict[\"params\", \"g\"]\n    mass = tensordict[\"params\", \"m\"]\n    length = tensordict[\"params\", \"l\"]\n    dt = tensordict[\"params\", \"dt\"]\n    u = tensordict[\"action\"].squeeze(-1)\n    u = u.clamp(-tensordict[\"params\", \"max_torque\"], tensordict[\"params\", \"max_torque\"])\n    costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)\n\n    new_thdot = (\n        thdot\n        + (3 * g_force / (2 * length) * th.sin() + 3.0 / (mass * length**2) * u) * dt\n    )\n    new_thdot = new_thdot.clamp(\n        -tensordict[\"params\", \"max_speed\"], tensordict[\"params\", \"max_speed\"]\n    )\n    new_th = th + new_thdot * dt\n    reward = -costs.view(*tensordict.shape, 1)\n    done = torch.zeros_like(reward, dtype=torch.bool)\n    out = TensorDict(\n        {\n            \"th\": new_th,\n            \"thdot\": new_thdot,\n            \"params\": tensordict[\"params\"],\n            \"reward\": reward,\n            \"done\": done,\n        },\n        tensordict.shape,\n    )\n    return out\n\n\ndef angle_normalize(x):\n    return ((x + torch.pi) % (2 * torch.pi)) - torch.pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resetting the simulator: :func:`torchrl.envs.EnvBase._reset`\n\nThe second method we need to care about is the\n:func:`torchrl.envs.EnvBase._reset` method. Like\n:func:`torchrl.envs.EnvBase._step`, it should write the observation entries\nand possibly a done state in the tensordict it outputs (if the done state is\nomitted, it will be filled as ``False`` by the parent method\n:func:`torchrl.envs.EnvBase.reset`). In some contexts, it is required that\nthe `_reset` method receives a command from the function that called\nit (e.g. in multi-agent settings we may want to indicate which agents need\nto be reset). This is why the :func:`torchrl.envs.EnvBase._reset` method\nalso expects a tensordict as input, albeit it may perfectly be empty or\n``None``.\n\nThe parent :class:`EnvBase.reset` does some simple checks like the\n:class:`EnvBase.step` does, such as making sure that a ``\"done\"`` state\nis returned in the output tensordict and that the shapes match what is\nexpected from the specs.\n\nFor us, the only important thing to consider is whether\n:class:`EnvBase._reset` contains all the expected observations. Once more,\nsince we are working with a stateless environment, we pass the configuration\nof the pendulum in a nested tensordict named ``\"params\"``.\n\nIn this example, we do not pass a done state as this is not mandatory\nfor :func:`_reset` and our environment is non-terminating so we always\nexpect it to be ``False``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _reset(self, tensordict):\n    if tensordict is None or tensordict.is_empty():\n        # if no tensordict is passed, we generate a single set of hyperparameters\n        # Otherwise, we assume that the input tensordict contains all the relevant\n        # parameters to get started.\n        tensordict = self.gen_params(batch_size=self.batch_size)\n\n    high_th = torch.tensor(DEFAULT_X, device=self.device)\n    high_thdot = torch.tensor(DEFAULT_Y, device=self.device)\n    low_th = -high_th\n    low_thdot = -high_thdot\n\n    # for non batch-locked envs, the input tensordict shape dictates the number\n    # of simulators run simultaneously. In other contexts, the initial\n    # random state's shape will depend upon the environment batch-size instead.\n    th = (\n        torch.rand(tensordict.shape, generator=self.rng, device=self.device)\n        * (high_th - low_th)\n        + low_th\n    )\n    thdot = (\n        torch.rand(tensordict.shape, generator=self.rng, device=self.device)\n        * (high_thdot - low_thdot)\n        + low_thdot\n    )\n    out = TensorDict(\n        {\n            \"th\": th,\n            \"thdot\": thdot,\n            \"params\": tensordict[\"params\"],\n        },\n        batch_size=tensordict.shape,\n    )\n    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment metadata: ``env.*_spec``\n\nThe specs define the input and output domain of the environment.\nIt is important that the specs accurately define the tensors that will be\nreceived at runtime, as they are often used to carry information about\nenvironments in multiprocessing and distributed settings. They can also be\nused to instantiate lazily defined neural networks and test scripts without\nactually querying the environment (which can be costly with real-world\nphysical systems for instance).\n\nThere are four specs that we must code in our environment:\n\n* :obj:`EnvBase.observation_spec`: This will be a :class:`torchrl.data.CompositeSpec`\n  instance where each key is an observation.\n* :obj:`EnvBase.action_spec`: It can be any type of spec, but it is required that it\n  corresponds to the ``\"action\"`` entry in the input tensordict.\n* :obj:`EnvBase.input_spec`: contains all the input entries,\n  including the :obj:`EnvBase.action_spec` (which is just a pointer to\n  :obj:`EnvBase.input_spec['action_spec']`. As for :obj:`EnvBase.ObservationSpec`,\n  it is expected that this spec is of type :obj:`torchrl.data.CompositeSpec`.\n  to accommodate environments where multiple inputs are expected.\n* :obj:`EnvBase.reward_spec`: the reward spec have the particularity of\n  having a singleton trailing dimension if the environment has an empty\n  batch size. The reason is that we often pass observations in torch models\n  that estimate a value estimate with non-empty shape:\n\n```\n>>> next_value = reward + (1 - done) * fun(observation)\n```\n  Working with *unsqueezed* rewards allows us to build algorithms that are\n  not polluted by squeezing and unsqueezing operations.\n\nTorchRL offers multiple :class:`torchrl.data.TensorSpec`\n[subclasses](https://pytorch.org/rl/reference/data.html#tensorspec) to\nencode the environment's input and output characteristics.\n\n### Specs shape\n\nThe environment specs leading dimensions must match the\nenvironment batch-size. This is done to enforce that every component of an\nenvironment (including its transforms) have an accurate representation of\nthe expected input and output shapes. This is something that should be\naccurately coded in stateful settings.\n\nFor non batch-locked environments such as the one in our example (see below),\nthis is irrelevant as the environment batch-size will most likely be empty.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _make_spec(self, td_params):\n    self.observation_spec = CompositeSpec(\n        th=BoundedTensorSpec(\n            minimum=-torch.pi,\n            maximum=torch.pi,\n            shape=(),\n            dtype=torch.float32,\n        ),\n        thdot=BoundedTensorSpec(\n            minimum=-td_params[\"params\", \"max_speed\"],\n            maximum=td_params[\"params\", \"max_speed\"],\n            shape=(),\n            dtype=torch.float32,\n        ),\n        # we need to add the \"params\" to the observation specs, as we want\n        # to pass it at each step during a rollout\n        params=make_composite_from_td(td_params[\"params\"]),\n        shape=(),\n    )\n    # since the environment is stateless, we expect the previous output as input\n    self.input_spec = self.observation_spec.clone()\n    # action-spec will be automatically wrapped in input_spec, but the convenient\n    # self.action_spec = spec is supported\n    self.action_spec = BoundedTensorSpec(\n        minimum=-td_params[\"params\", \"max_torque\"],\n        maximum=td_params[\"params\", \"max_torque\"],\n        shape=(1,),\n        dtype=torch.float32,\n    )\n    self.reward_spec = UnboundedContinuousTensorSpec(shape=(*td_params.shape, 1))\n\n\ndef make_composite_from_td(td):\n    # custom funtion to convert a tensordict in a similar spec structure\n    # of unbounded values.\n    composite = CompositeSpec(\n        {\n            key: make_composite_from_td(tensor)\n            if isinstance(tensor, TensorDictBase)\n            else UnboundedContinuousTensorSpec(\n                dtype=tensor.dtype, device=tensor.device, shape=tensor.shape\n            )\n            for key, tensor in td.items()\n        },\n        shape=td.shape,\n    )\n    return composite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reproducible experiments: seeding\n\nSeeding an environment is a commong operation when initializing an experiment.\n:func:`EnvBase._set_seed` only goal is to set the seed of the contained\nsimulator. If possible, this operation should not call `reset()` or interact\nwith the environment execution. The parent :func:`EnvBase.set_seed` method\nincorporates a mechanism that allows seeding multiple environments with a\ndifferent pseudo-random and reproducible seed.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _set_seed(self, seed: Optional[int]):\n    rng = torch.manual_seed(seed)\n    self.rng = rng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrapping things together: the :class:`torchrl.envs.EnvBase` class\n\nWe can finally put together the pieces and design our environment class.\nThe specs initialization needs to be performed during the environment\nconstruction, so we must take care of calling the :func:`_make_spec` method\nwithin :func:`PendulumEnv.__init__`.\n\nWe add a static method :func:`PendulumEnv.gen_params` which deterministically\ngenerates a set of hyperparameters to be used during execution:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gen_params(g=10.0, batch_size=None) -> TensorDictBase:\n    \"\"\"Returns a tensordict containing the physical parameters such as gravitational force and torque or speed limits.\"\"\"\n    if batch_size is None:\n        batch_size = []\n    td = TensorDict(\n        {\n            \"params\": TensorDict(\n                {\n                    \"max_speed\": 8,\n                    \"max_torque\": 2.0,\n                    \"dt\": 0.05,\n                    \"g\": g,\n                    \"m\": 1.0,\n                    \"l\": 1.0,\n                },\n                [],\n            )\n        },\n        [],\n    )\n    if batch_size:\n        td = td.expand(batch_size).contiguous()\n    return td"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the environment as non-``batch_locked`` by turning the homonymous\nattribute to ``False``. This means that we will **not** enforce the input\ntensordict to have a batch-size that matches the one of the environment.\n\nThe following code will just put together the pieces we have coded above.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PendulumEnv(EnvBase):\n    metadata = {\n        \"render_modes\": [\"human\", \"rgb_array\"],\n        \"render_fps\": 30,\n    }\n    batch_locked = False\n\n    def __init__(self, td_params=None, seed=None, device=\"cpu\"):\n        if td_params is None:\n            td_params = self.gen_params()\n\n        super().__init__(device=device, batch_size=[])\n        self._make_spec(td_params)\n        if seed is None:\n            seed = torch.empty((), dtype=torch.int64).random_().item()\n        self.set_seed(seed)\n\n    # Helpers: _make_step and gen_params\n    gen_params = staticmethod(gen_params)\n    _make_spec = _make_spec\n\n    # Mandatory methods: _step, _reset and _set_seed\n    _reset = _reset\n    _step = staticmethod(_step)\n    _set_seed = _set_seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing our environment\n\nTorchRL provides a simple function :func:`torchrl.envs.utils.check_env_specs`\nto check that a (transformed) environment has an input/output structure that\nmatches the one dictated by its specs.\nLet us try it out:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = PendulumEnv()\ncheck_env_specs(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can have a look at our specs to have a visual representation of the environment\nsignature:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"observation_spec:\", env.observation_spec)\nprint(\"input_spec:\", env.input_spec)\nprint(\"reward_spec:\", env.reward_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can execute a couple of commands too to check that the output structure\nmatches what is expected.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "td = env.reset()\nprint(\"reset tensordict\", td)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can run the :func:`env.rand_step` to generate\nan action randomly from the ``action_spec`` domain. A tensordict containing\nthe hyperparams and the current state **must** be passed since our\nenvironment is stateless. In stateful contexts, ``env.rand_step()`` works\nperfectly too.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "td = env.rand_step(td)\nprint(\"random step tensordict\", td)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforming an environment\n\nWriting environment transforms for stateless simulators is slightly more\ncomplicated than for stateful ones: transforming an output entry that needs\nto be read at the following iteration requires to apply the inverse transform\nbefore calling :func:`env.step` at the next step.\nThis is an ideal scenario to showcase all the features of torchrl's\ntransforms!\n\nFor instance, in the following transformed environment we unsqueeze the entries\n``[\"th\", \"thdot\"]`` to be able to stack them along the last\ndimension. We also pass them as ``in_keys_inv`` to squeeze them back to their\noriginal shape once they are passed as input in the next iteration.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = TransformedEnv(\n    env,\n    # Unsqueezes the observations that we will concatenate\n    UnsqueezeTransform(\n        unsqueeze_dim=-1,\n        in_keys=[\"th\", \"thdot\"],\n        in_keys_inv=[\"th\", \"thdot\"],\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Writing custom transforms\n\nTorchRL's transforms may not cover all the operations one wants to execute\nafter an environment has been executed.\nWriting a transform does not require much effort. As for the environment\ndesign, there are two steps in writing a transform:\n\n- Getting the dynamics right (forward and inverse);\n- Adapting the environment specs.\n\nA transform can be used in two settings: on its own, it can be used as a\n:class:`torch.nn.Module`. It can also be used appended to a\n:class:`torchrl.envs.TransformedEnv`. The structure of the class allows to\ncustomize the behaviour in the different contexts.\n\nA :class:`torchrl.envs.Transform` skeleton can be summarized as follows:\n\n```\nclass Transform(nn.Module):\n    def forward(self, tensordict):\n    def _apply_transform(self, tensordict):\n    def _step(self, tensordict):\n    def _call(self, tensordict):\n    def inv(self, tensordict):\n    def _inv_apply_transform(self, tensordict):\n```\nThere are three entry points (:func:`forward`, :func:`_step` and :func:`inv`)\nwhich all receive :class:`tensordict.TensorDict` instances. The first two\nwill eventually go through the keys indicated by :obj:`Transform.in_keys`\nand call :func:`Transform._apply_transform` to each of these. The results will\nbe written in the entries pointed by :obj:`Transform.out_keys` if provided\n(if not the ``in_keys`` will be updated with the transformed values).\nIf inverse transforms need to be executed, a similar data flow will be\nexecuted but with the :func:`Transform.inv` and\n:func:`Transform._inv_apply_transform` methods and across the ``in_keys_inv``\nand ``out_keys_inv`` list of keys.\nThe following figure summarized this flow for environments and replay\nbuffers.\n\n.. figure:: /_static/img/transforms.png\n\n   Transform API\n\nIn some cases, a transform will not work on a subset of keys in a unitary\nmanner, but will execute some operation on the parent environment or\nwork with the entire input tensordict.\nIn those cases, the :func:`_call` and :func:`forward` methods should be\nre-written, and the :func:`_apply_transform` method can be skipped.\n\nLet us code new transforms that will compute the ``sine`` and ``cosine``\nvalues of the position angle, as these values are more useful to us to learn\na policy than the raw angle value:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SinTransform(Transform):\n    def _apply_transform(self, obs: torch.Tensor) -> None:\n        return obs.sin()\n\n    # _apply_to_composite will execute the observation spec transform across all\n    # in_keys/out_keys pairs and write the result in the observation_spec which\n    # is of type ``Composite``\n    @_apply_to_composite\n    def transform_observation_spec(self, observation_spec):\n        return BoundedTensorSpec(\n            minimum=-1,\n            maximum=1,\n            shape=observation_spec.shape,\n            dtype=observation_spec.dtype,\n            device=observation_spec.device,\n        )\n\n\nclass CosTransform(Transform):\n    def _apply_transform(self, obs: torch.Tensor) -> None:\n        return obs.cos()\n\n    # _apply_to_composite will execute the observation spec transform across all\n    # in_keys/out_keys pairs and write the result in the observation_spec which\n    # is of type ``Composite``\n    @_apply_to_composite\n    def transform_observation_spec(self, observation_spec):\n        return BoundedTensorSpec(\n            minimum=-1,\n            maximum=1,\n            shape=observation_spec.shape,\n            dtype=observation_spec.dtype,\n            device=observation_spec.device,\n        )\n\n\nt_sin = SinTransform(in_keys=[\"th\"], out_keys=[\"sin\"])\nt_cos = CosTransform(in_keys=[\"th\"], out_keys=[\"cos\"])\nenv.append_transform(t_sin)\nenv.append_transform(t_cos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Concatenates the observations onto an \"observation\" entry.\ndel_keys=False ensures that we keep these values for the next\niteration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cat_transform = CatTensors(\n    in_keys=[\"sin\", \"cos\", \"thdot\"], dim=-1, out_key=\"observation\", del_keys=False\n)\nenv.append_transform(cat_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once more, let us check that our env specs match what is received:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check_env_specs(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executing a rollout\n\nExecuting a rollout is a succession of simple steps:\n\n* reset the environment\n* while some condition is not met:\n\n  * compute an action given a policy\n  * execute a step given this action\n  * collect the data\n  * make a MDP step\n\n* gather the data and return\n\nThese operations have been convinently wrapped in the :func:`EnvBase.rollout`\nmethod, from which we provide a simplified version here below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def simple_rollout(steps=100):\n    # preallocate:\n    data = TensorDict({}, [steps])\n    # reset\n    _data = env.reset()\n    for i in range(steps):\n        _data[\"action\"] = env.action_spec.rand()\n        _data = env.step(_data)\n        data[i] = _data\n        _data = step_mdp(_data, keep_other=True)\n    return data\n\n\nprint(\"data from rollout:\", simple_rollout(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching computations\n\nThe last unexplored end of our tutorial is the ability that we have to\nbatch computations in TorchRL. Because our environment does not\nmake any assumptions regarding the input data shape, we can seamlessly\nexecute it over batches of data. Even better: for non-batch-locked\nenvironments such as our Pendulum, we can change the batch size on the fly\nwithout recreating the env.\nTo do this, we just generate parameters with the desired shape.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 10  # number of environments to be executed in batch\ntd = env.reset(env.gen_params(batch_size=[batch_size]))\nprint(\"reset (batch size of 10)\", td)\ntd = env.rand_step(td)\nprint(\"rand step (batch size of 10)\", td)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "executing a rollout with a batch of data requires us to reset the env\nout of the rollout function, since we need to define the batch_size\ndynamically and this is not supported by :func:`EnvBase.rollout`:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rollout = env.rollout(\n    3,\n    auto_reset=False,  # we're executing the reset out of the ``rollout`` call\n    tensordict=env.reset(env.gen_params(batch_size=[batch_size])),\n)\nprint(\"rollout of len 3 (batch size of 10):\", rollout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a simple policy\n\nIn this example, we will train a simple policy using the reward as a\ndifferentiable objective (i.e. a negative loss).\nWe will take advantage of the fact that our dynamic system is fully\ndifferentiable to backpropagate through the trajectory return and adjust the\nweights of our policy to maximise this value directly. Of course, in many\nsettings many of the assumptions we make do not hold, such as\ndifferentiability of the system and full access to the underlying mechanics.\n\nStill, this is a very simple example that showcases how a training loop can\nbe coded with a custom environment in TorchRL.\n\nLet us first write the policy network:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\nenv.set_seed(0)\n\nnet = nn.Sequential(\n    nn.LazyLinear(64),\n    nn.Tanh(),\n    nn.LazyLinear(64),\n    nn.Tanh(),\n    nn.LazyLinear(64),\n    nn.Tanh(),\n    nn.LazyLinear(1),\n)\npolicy = TensorDictModule(\n    net,\n    in_keys=[\"observation\"],\n    out_keys=[\"action\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and our optimizer:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(policy.parameters(), lr=2e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop\n\nWe will successively:\n\n* generate a trajectory\n* sum the rewards\n* backpropagate through the graph defined by these operations\n* clip the gradient norm and make an optimization step\n* repeat\n\nAt the end of the training loop, we should have a final reward close to 0\nwhich demonstrates that the pendulum is upward and still as desired.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 32\npbar = tqdm.tqdm(range(20_000 // batch_size))\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 20_000)\nlogs = defaultdict(list)\n\nfor _ in pbar:\n    init_td = env.reset(env.gen_params(batch_size=[batch_size]))\n    rollout = env.rollout(100, policy, tensordict=init_td, auto_reset=False)\n    traj_return = rollout[\"reward\"].mean()\n    (-traj_return).backward()\n    gn = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n    optim.step()\n    optim.zero_grad()\n    pbar.set_description(\n        f\"reward: {traj_return: 4.4f}, \"\n        f\"last reward: {rollout[..., -1]['reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}\"\n    )\n    logs[\"return\"].append(traj_return.item())\n    logs[\"last_reward\"].append(rollout[..., -1][\"reward\"].mean().item())\n    scheduler.step()\n\n\ndef plot():\n    import matplotlib\n    from matplotlib import pyplot as plt\n\n    is_ipython = \"inline\" in matplotlib.get_backend()\n    if is_ipython:\n        from IPython import display\n\n    with plt.ion():\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.plot(logs[\"return\"])\n        plt.title(\"returns\")\n        plt.xlabel(\"iteration\")\n        plt.subplot(1, 2, 2)\n        plt.plot(logs[\"last_reward\"])\n        plt.title(\"last reward\")\n        plt.xlabel(\"iteration\")\n        if is_ipython:\n            display.display(plt.gcf())\n            display.clear_output(wait=True)\n        plt.show()\n\n\nplot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this tutorial, we have learned how to code a stateless environment from\nscratch. We touched the subjects of:\n\n* the four essential components that need to be taken care of when coding\n  an environment (:func:`step`, :func:`reset\", seeding and building specs).\n  We saw how these methods and classes interact with the\n  :class:`tensordict.TensorDict` class;\n* how to test that an environment is properly coded using\n  :func:`torchrl.envs.utils.check_env_specs`;\n* How to append transforms in the context of stateless environments and how\n  to write custom transformations;\n* How to train a policy on a fully differentiable simulator.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}